# ğŸ“Š PrÃ©diction des coÃ»ts ultimes par rÃ©gression supervisÃ©e

Projet rÃ©alisÃ© dans le cadre du Master 1 EconomÃ©trie, statistiques â€“ Introduction Ã  lâ€™Apprentissage SupervisÃ©.

## ğŸ¯ Objectif
DÃ©velopper un modÃ¨le de machine learning capable de prÃ©dire les coÃ»ts ultimes dâ€™accidents Ã  partir de donnÃ©es socio-professionnelles et temporelles. Lâ€™approche repose sur :
- Une prÃ©paration rigoureuse des donnÃ©es
- Un feature engineering avancÃ©
- La comparaison de plusieurs modÃ¨les de rÃ©gression

## ğŸ“ Contenu du projet
- `ML_Notebook.ipynb` : Notebook principal contenant le code, les visualisations et les explications
- `train_etu.xlsx` : Jeu de donnÃ©es d'entraÃ®nement

## ğŸ§ª MÃ©thodologie

### 1. PrÃ©traitement des donnÃ©es
- Imputation des valeurs manquantes (moyenne pour les numÃ©riques, mode pour les catÃ©goriques)
- DÃ©tection et suppression des outliers via IQR et Isolation Forest
- Encodage des variables catÃ©gorielles (OneHotEncoder)
- Normalisation des variables numÃ©riques (StandardScaler)

### 2. Feature Engineering
- Transformation des dates (`date_diff`, `delai_communication`, `weekend_accident`)
- CrÃ©ation de variables quadratiques (`ageÂ²`, `cout_initÂ²`)
- Termes dâ€™interaction (`salaire_semaine Ã— cout_init`, `age Ã— cout_init`, etc.)
- Traitement du texte avec `TfidfVectorizer` sur la variable `description`

### 3. ModÃ©lisation
ModÃ¨les testÃ©s avec pipelines :
- RÃ©gression linÃ©aire
- Arbre de dÃ©cision
- Random Forest (500 arbres)
- Gradient Boosting (500 arbres)

### 4. Ã‰valuation
- **RMSE** (Root Mean Squared Error)
- **RÂ²** (Coefficient de dÃ©termination)
- Temps dâ€™infÃ©rence

| ModÃ¨le              | RMSE     | RÂ²     | Temps d'infÃ©rence |
|---------------------|----------|--------|-------------------|
| RÃ©gression linÃ©aire | 10â€¯268   | 0.5313 | 0.04 s            |
| Arbre de dÃ©cision   | 16â€¯423   | -0.198 | 0.02 s            |
| Random Forest       | 10â€¯699   | 0.528  | 1.24 s            |
| Gradient Boosting   | **10â€¯261** | **0.5320** | 0.04 s        |

## ğŸ† Conclusion
Le modÃ¨le de **Gradient Boosting** est le plus performant en termes de prÃ©cision. La rÃ©gression linÃ©aire reste une alternative simple et interprÃ©table. Lâ€™arbre de dÃ©cision nÃ©cessite des ajustements pour Ãªtre compÃ©titif.

## ğŸ“š Technologies utilisÃ©es
- Python (pandas, numpy, scikit-learn, matplotlib, seaborn, plotly)
- PyOD pour la dÃ©tection dâ€™anomalies
- TfidfVectorizer pour le traitement du texte

